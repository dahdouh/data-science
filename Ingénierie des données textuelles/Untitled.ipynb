{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"L'analyse syntaxique aussi bien que l'analyse 5061 en linguistique ont pour finalité de caractériser l'énoncé dans son ensemble, \n",
      "principalement par la détermination des structures de l'énoncé.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "document = \"\"\"\"L'analyse syntaxique aussi bien que l'analyse 5061 en linguistique ont pour finalité de caractériser l'énoncé dans son ensemble, \n",
    "principalement par la détermination des structures de l'énoncé.\n",
    "Dans les deux cas, la détermination des structures repose sur une caractérisation de ses éléments de base, \n",
    "les mots, et leurs propres constituants, \n",
    "mais de façon différente selon ces deux approches.\"\"\"\n",
    "phrases = sent_tokenize(document)\n",
    "print(phrases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', \"L'analyse\", 'syntaxique', 'aussi', 'bien', 'que', \"l'analyse\", 'sémantique', 'en', 'linguistique', 'ont', 'pour', 'finalité', 'de', 'caractériser', \"l'énoncé\", 'dans', 'son', 'ensemble', ',', 'principalement', 'par', 'la', 'détermination', 'des', 'structures', 'de', \"l'énoncé\", '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(phrases[0])\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', \"L'ANALYSE\", 'SYNTAXIQUE', 'AUSSI', 'BIEN', 'QUE', \"L'ANALYSE\", 'SÉMANTIQUE', 'EN', 'LINGUISTIQUE', 'ONT', 'POUR', 'FINALITÉ', 'DE', 'CARACTÉRISER', \"L'ÉNONCÉ\", 'DANS', 'SON', 'ENSEMBLE', ',', 'PRINCIPALEMENT', 'PAR', 'LA', 'DÉTERMINATION', 'DES', 'STRUCTURES', 'DE', \"L'ÉNONCÉ\", '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.upper() for w in tokens]\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convertir un nombre numérique en mot\n",
      "['5061']\n",
      "Nombre après conversion \n",
      "\n",
      "['five thousand and sixty-one']\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "tokens = word_tokenize(phrases[0])\n",
    "\n",
    "print(\"convertir un nombre numérique en mot\")\n",
    "word = [word for word in tokens if word.isdigit()]\n",
    "print(word)\n",
    "\n",
    "p = inflect.engine()\n",
    "numbertransf = [p.number_to_words(word) for word in tokens if word.isdigit()]\n",
    "\n",
    "print (\"Nombre après conversion \\n\")\n",
    "print(numbertransf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syntaxique', 'aussi', 'bien', 'que', 'en', 'linguistique', 'ont', 'pour', 'finalité', 'de', 'caractériser', 'dans', 'son', 'ensemble', 'principalement', 'par', 'la', 'détermination', 'des', 'structures', 'de']\n"
     ]
    }
   ],
   "source": [
    "# Suppression de tous les termes qui ne sont pas alphanumériques\n",
    "words_alpha = [word for word in tokens if word.isalpha()]\n",
    "print(words_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/karim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "#stop_words = stopwords.words('english')\n",
    "#print(stop_words)\n",
    "stop_words = stopwords.words('french')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'application des stopwords\n",
      "\n",
      "Avant transformation \n",
      "\n",
      "['syntaxique', 'aussi', 'bien', 'que', 'en', 'linguistique', 'ont', 'pour', 'finalité', 'de', 'caractériser', 'dans', 'son', 'ensemble', 'principalement', 'par', 'la', 'détermination', 'des', 'structures', 'de']\n",
      "\n",
      " Après transformation \n",
      "\n",
      "['syntaxique', 'aussi', 'bien', 'linguistique', 'finalité', 'caractériser', 'ensemble', 'principalement', 'détermination', 'structures']\n"
     ]
    }
   ],
   "source": [
    "print (\"Exemple d'application des stopwords\\n\")\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print (\"Avant transformation \\n\")\n",
    "print (words)\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print (\"\\n Après transformation \\n\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['syntaxiqu', 'aussi', 'bien', 'que', 'en', 'linguistiqu', 'ont', 'pour', 'finalité', 'de', 'caractéris', 'dan', 'son', 'ensembl', 'principal', 'par', 'la', 'détermin', 'de', 'structur', 'de']\n"
     ]
    }
   ],
   "source": [
    "#Stemmatisation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation \n",
      "\n",
      "Lemmatisation : \n",
      " ['``', \"l'analyse\", 'syntaxique', 'aussi', 'bien', 'que', \"l'analyse\", '5061', 'en', 'linguistique', 'ont', 'pour', 'finalité', 'de', 'caractériser', \"l'énoncé\", 'dans', 'son', 'ensemble', ',', 'principalement', 'par', 'la', 'détermination', 'des', 'structure', 'de', \"l'énoncé\", '.']\n",
      "Lemmatisation : \n",
      " ['``', \"l'analyse\", 'syntaxique', 'aussi', 'bien', 'que', \"l'analyse\", '5061', 'en', 'linguistique', 'ont', 'pour', 'finalité', 'de', 'caractériser', \"l'énoncé\", 'dans', 'son', 'ensemble', ',', 'principalement', 'par', 'la', 'détermination', 'des', 'structure', 'de', \"l'énoncé\", '.']\n",
      "\n",
      " autre exemple avec la phrase\n",
      "\n",
      "have had having dogs crying\n"
     ]
    }
   ],
   "source": [
    "## Lemmatisation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "print (\"Lemmatisation \\n\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lstemmed = [wordnet_lemmatizer.lemmatize(word,pos='v') for word in tokens]\n",
    "print(\"Lemmatisation : \\n\",lstemmed)\n",
    "\n",
    "\n",
    "print(\"Lemmatisation : \\n\",lstemmed)\n",
    "print (\"\\n autre exemple avec la phrase\\n\")\n",
    "sentence = \"have had having dogs crying\"\n",
    "print (sentence)\n",
    "tokens = word_tokenize(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/karim/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /home/karim/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('``', '``'),\n",
       " (\"l'analyse\", 'NN'),\n",
       " ('syntaxique', 'NN'),\n",
       " ('aussi', 'JJ'),\n",
       " ('bien', 'NN'),\n",
       " ('que', 'NN'),\n",
       " (\"l'analyse\", 'NN'),\n",
       " ('5061', 'CD'),\n",
       " ('en', 'NN'),\n",
       " ('linguistique', 'NN'),\n",
       " ('ont', 'IN'),\n",
       " ('pour', 'JJ'),\n",
       " ('finalité', 'NN'),\n",
       " ('de', 'IN'),\n",
       " ('caractériser', 'FW'),\n",
       " (\"l'énoncé\", 'JJ'),\n",
       " ('dans', 'NNS'),\n",
       " ('son', 'VBP'),\n",
       " ('ensemble', 'JJ'),\n",
       " (',', ','),\n",
       " ('principalement', 'JJ'),\n",
       " ('par', 'NN'),\n",
       " ('la', 'NN'),\n",
       " ('détermination', 'NN'),\n",
       " ('des', 'FW'),\n",
       " ('structures', 'FW'),\n",
       " ('de', 'FW'),\n",
       " (\"l'énoncé\", 'FW'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "tokens = word_tokenize(phrases[0])\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('``', '``'), (\"L'analyse\", 'NNP'), ('syntaxique', 'NN'), ('aussi', 'NN'), ('bien', 'NN'), ('que', 'NN'), (\"l'analyse\", 'NN'), ('5061', 'CD'), ('en', 'NN'), ('linguistique', 'NN'), ('ont', 'IN'), ('pour', 'JJ'), ('finalité', 'NN'), ('de', 'IN'), ('caractériser', 'FW'), (\"l'énoncé\", 'JJ'), ('dans', 'NNS'), ('son', 'VBP'), ('ensemble', 'JJ'), (',', ','), ('principalement', 'JJ'), ('par', 'NN'), ('la', 'NN'), ('détermination', 'NN'), ('des', 'FW'), ('structures', 'FW'), ('de', 'FW'), (\"l'énoncé\", 'FW'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokens = word_tokenize(phrases[0])\n",
    "\n",
    "tokens=pos_tag(tokens)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liste des topics \n",
      "\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "from pprint import pprint\n",
    "print (\"liste des topics \\n\")\n",
    "pprint(list(news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
